{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5d7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66820758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion \n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader, Docx2txtLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "# loader = UnstructuredWordDocumentLoader('../Data/EU AI Act Doc (1) (3).docx',\n",
    "loader = Docx2txtLoader('../Data/EU AI Act Doc (1) (3).docx', \n",
    "                #   mode=\"elements\"\n",
    "                  )\n",
    "text_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86acee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='High-level summary of the AI Act\\n\\n27 Feb, 2024\\n\\nUpdated on 30 May in accordance with the Corrigendum version of the AI Act.\\n\\nIn this article we provide you with a high-level summary of the AI Act, selecting the parts which are most likely to be relevant to you regardless of who you are. We provide links to the original document where relevant so that you can always reference the Act text.\\n\\nTo explore the full text of the AI Act yourself, use our\\xa0AI Act Explorer. Alternatively, if you want to know which parts of the text are most relevant to you, use our\\xa0Compliance Checker. \\n\\nFour-point summary\\n\\nThe AI Act classifies AI according to its risk:\\n\\nUnacceptable risk is prohibited (e.g. social scoring systems and manipulative AI).\\n\\nMost of the text addresses high-risk AI systems, which are regulated.\\n\\nA smaller section handles limited risk AI systems, subject to lighter transparency obligations: developers and deployers must ensure that end-users are aware that they are interacting with AI (chatbots and deepfakes).\\n\\nMinimal risk is unregulated (including the majority of AI applications currently available on the EU single market, such as AI enabled video games and spam filters – at least in 2021; this is changing with generative AI).\\n\\nThe majority of obligations fall on providers (developers) of high-risk AI systems.\\n\\nThose that intend to place on the market or put into service high-risk AI systems in the EU, regardless of whether they are based in the EU or a third country.\\n\\nAnd also third country providers where the high risk AI system’s output is used in the EU.\\n\\nUsers are natural or legal persons that deploy an AI system in a professional capacity, not affected end-users.\\n\\nUsers (deployers) of high-risk AI systems have some obligations, though less than providers (developers).\\n\\nThis applies to users located in the EU, and third country users where the AI system’s output is used in the EU.\\n\\nGeneral purpose AI (GPAI):\\n\\nAll GPAI model providers must provide technical documentation, instructions for use, comply with the Copyright Directive, and publish a summary about the content used for training.\\n\\nFree and open licence GPAI model providers only need to comply with copyright and publish the training data summary, unless they present a systemic risk.\\n\\nAll providers of GPAI models that present a systemic risk – open or closed – must also conduct model evaluations, adversarial testing, track and report serious incidents and ensure cybersecurity protections.\\n\\nProhibited AI systems (Chapter II,\\xa0Art. 5)\\n\\nThe following types of AI system are ‘Prohibited’ according to the AI Act.\\n\\nAI systems:\\n\\ndeploying\\xa0subliminal, manipulative, or deceptive techniques\\xa0to distort behaviour and impair informed decision-making, causing significant harm.\\n\\nexploiting vulnerabilities\\xa0related to age, disability, or socio-economic circumstances to distort behaviour, causing significant harm.\\n\\nbiometric categorisation systems\\xa0inferring sensitive attributes (race, political opinions, trade union membership, religious or philosophical beliefs, sex life, or sexual orientation), except labelling or filtering of lawfully acquired biometric datasets or when law enforcement categorises biometric data.\\n\\nsocial scoring, i.e., evaluating or classifying individuals or groups based on social behaviour or personal traits, causing detrimental or unfavourable treatment of those people.\\n\\nassessing the risk of an individual committing criminal offenses\\xa0solely based on profiling or personality traits, except when used to augment human assessments based on objective, verifiable facts directly linked to criminal activity.\\n\\ncompiling facial recognition databases\\xa0by untargeted scraping of facial images from the internet or CCTV footage.\\n\\ninferring emotions in workplaces or educational institutions, except for medical or safety reasons.\\n\\n‘real-time’ remote biometric identification (RBI) in publicly accessible spaces for law enforcement, except when:\\n\\nsearching for missing persons, abduction victims, and people who have been human trafficked or sexually exploited;\\n\\npreventing substantial and imminent threat to life, or foreseeable terrorist attack; or\\n\\nidentifying suspects in serious crimes (e.g., murder, rape, armed robbery, narcotic and illegal weapons trafficking, organised crime, and environmental crime, etc.).\\n\\nNotes on remote biometric identification:\\n\\nUsing AI-enabled real-time RBI is only allowed\\xa0when not using the tool would cause considerable harm\\xa0and must account for affected persons’ rights and freedoms.\\n\\nBefore deployment, police must complete a\\xa0fundamental rights impact assessment\\xa0and\\xa0register the system in the EU database, though, in duly justified cases of urgency, deployment can commence without registration, provided that it is registered later without undue delay.\\n\\nBefore deployment, they also must obtain\\xa0authorisation from a judicial authority or independent administrative authority[1], though, in duly justified cases of urgency, deployment can commence without authorisation, provided that authorisation is requested within 24 hours. If authorisation is rejected, deployment must cease immediately, deleting all data, results, and outputs.\\n\\n↲\\xa0[1] Independent administrative authorities may be subject to greater political influence than judicial authorities (Hacker, 2024).\\n\\nHigh risk AI systems (Chapter III)\\n\\nSome AI systems are considered ‘High risk’ under the AI Act. Providers of those systems will be subject to additional requirements.\\n\\nClassification rules for high-risk AI systems (Art. 6)\\n\\nHigh risk AI systems are those:\\n\\nused as a safety component or a product covered by EU laws in\\xa0Annex I\\xa0AND\\xa0required to undergo a third-party conformity assessment under those\\xa0Annex I\\xa0laws;\\xa0OR\\n\\nthose under\\xa0Annex III\\xa0use cases (below), except if:\\n\\nthe AI system performs a narrow procedural task;\\n\\nimproves the result of a previously completed human activity;\\n\\ndetects decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment without proper human review; or\\n\\nperforms a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III.\\n\\nAI systems are always considered high-risk if it profiles individuals, i.e. automated processing of personal\\ndata to assess various aspects of a person’s life, such as work performance, economic situation, health,\\npreferences, interests, reliability, behaviour, location or movement.\\n\\nProviders whose AI system falls under the use cases in\\xa0Annex III\\xa0but believes it is\\xa0not\\xa0high-risk must document such an\\nassessment before placing it on the market or putting it into service.\\n\\nRequirements for providers of high-risk AI systems (Art.\\xa08–17)\\n\\nHigh risk AI providers must:\\n\\nEstablish a\\xa0risk management system\\xa0throughout the high risk AI system’s lifecycle;\\n\\nConduct\\xa0data governance, ensuring that training, validation and testing datasets are relevant, sufficiently representative and, to the best extent possible, free of errors and complete according to the intended purpose.\\n\\nDraw up\\xa0technical documentation\\xa0to demonstrate compliance and provide authorities with the information to assess that compliance.\\n\\nDesign their high risk AI system for\\xa0record-keeping\\xa0to enable it to automatically record events relevant for identifying national level risks and substantial modifications throughout the system’s lifecycle.\\n\\nProvide\\xa0instructions for use\\xa0to downstream deployers to enable the latter’s compliance.\\n\\nDesign their high risk AI system to allow deployers to implement\\xa0human oversight.\\n\\nDesign their high risk AI system to achieve appropriate levels of\\xa0accuracy, robustness, and cybersecurity.\\n\\nEstablish a\\xa0quality management system\\xa0to ensure compliance.\\n\\nAnnex III\\xa0use cases\\n\\nNon-banned biometrics:\\xa0Remote biometric identification systems, excluding biometric verification that confirm a person is who they claim to be. Biometric categorisation systems inferring sensitive or protected attributes or characteristics. Emotion recognition systems.\\n\\nCritical infrastructure:\\xa0Safety components in the management and operation of critical digital infrastructure, road traffic and the supply of water, gas, heating and electricity.\\n\\nEducation and vocational training:\\xa0AI systems determining access, admission or assignment to educational and vocational training institutions at all levels. Evaluating learning outcomes, including those used to steer the student’s learning process. Assessing the appropriate level of education for an individual. Monitoring and detecting prohibited student behaviour during tests.\\n\\nEmployment, workers management and access to self-employment:\\xa0AI systems used for recruitment or selection, particularly targeted job ads, analysing and filtering applications, and evaluating candidates. Promotion and termination of contracts, allocating tasks based on personality traits or characteristics and behaviour, and monitoring and evaluating performance.\\n\\nAccess to and enjoyment of essential public and private services:\\xa0AI systems used by public authorities for assessing eligibility to benefits and services, including their allocation, reduction, revocation, or recovery. Evaluating creditworthiness, except when detecting financial fraud. Evaluating and classifying emergency calls, including dispatch prioritising of police, firefighters, medical aid and urgent patient triage services. Risk assessments and pricing in health and life insurance.\\n\\nLaw enforcement:\\xa0\\xa0AI systems used to assess an individual’s risk of becoming a crime victim. Polygraphs. Evaluating evidence reliability during criminal investigations or prosecutions. Assessing an individual’s risk of offending or re-offending not solely based on profiling or assessing personality traits or past criminal behaviour. Profiling during criminal detections, investigations or prosecutions.\\n\\nMigration, asylum and border control management:\\xa0\\xa0Polygraphs. Assessments of irregular migration or health risks. Examination of applications for asylum, visa and residence permits, and associated complaints related to eligibility. Detecting, recognising or identifying individuals, except verifying travel documents.\\n\\nAdministration of justice and democratic processes:\\xa0\\xa0AI systems used in researching and interpreting facts and applying the law to concrete facts or used in alternative dispute resolution. Influencing elections and referenda outcomes or voting behaviour, excluding outputs that do not directly interact with people, like tools used to organise, optimise and structure political campaigns.\\n\\nGeneral purpose AI (GPAI)\\n\\nGPAI model\\xa0means an AI model, including when trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable to competently perform a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications. This does not cover AI models that are used before release on the market for research, development and prototyping activities.\\n\\nGPAI system\\xa0means an AI system which is based on a general purpose AI model, that has the capability to serve a variety of purposes, both for direct use as well as for integration in other AI systems.\\n\\nGPAI systems may be used as high risk AI systems or integrated into them. GPAI system providers should cooperate with such high risk AI system providers to enable the latter’s compliance.\\n\\nAll providers of GPAI models must:\\n\\nDraw up\\xa0technical documentation, including training and testing process and evaluation results.\\n\\nDraw up\\xa0information and documentation to supply to downstream providers\\xa0that intend to integrate the GPAI model into their own AI system in order that the latter understands capabilities and limitations and is enabled to comply.\\n\\nEstablish a policy to\\xa0respect the Copyright Directive.\\n\\nPublish a\\xa0sufficiently detailed summary about the content used for training\\xa0the GPAI model.\\n\\nFree and open licence GPAI models\\xa0– whose parameters, including weights, model architecture and model usage are publicly available, allowing for access, usage, modification and distribution of the model – only have to comply with the latter two obligations above, unless the free and open licence GPAI model is systemic.\\n\\nGPAI models present systemic risks when the cumulative amount of compute used for its training is greater than 1025\\xa0floating point operations (FLOPs). Providers must notify the Commission if their model meets this criterion within 2 weeks. The provider may present arguments that, despite meeting the criteria, their model does not present systemic risks. The Commission may decide on its own, or via a qualified alert from the scientific panel of independent experts, that a model has high impact capabilities, rendering it systemic.\\n\\nIn addition to the four obligations above, providers of GPAI models with systemic risk must also:\\n\\nPerform\\xa0model evaluations, including conducting and documenting\\xa0adversarial testing\\xa0to identify and mitigate systemic risk.\\n\\nAssess and mitigate possible systemic risks, including their sources.\\n\\nTrack, document and report serious incidents\\xa0and possible corrective measures to the\\xa0AI Office\\xa0and relevant national competent authorities without undue delay.\\n\\nEnsure an adequate level of\\xa0cybersecurity protection.\\n\\nAll GPAI model providers may demonstrate compliance with their obligations if they voluntarily adhere to a code of practice until European harmonised standards are published, compliance with which will lead to a presumption of conformity. Providers that don’t adhere to codes of practice must demonstrate\\xa0alternative adequate means of\\xa0compliance\\xa0for Commission approval.\\n\\nCodes of practice\\n\\nWill account for international approaches.\\n\\nWill cover but not necessarily limited to the above obligations, particularly the relevant information to include in technical documentation for authorities and downstream providers, identification of the type and nature of systemic risks and their sources, and the modalities of risk management accounting for specific challenges in addressing risks due to the way they may emerge and materialise throughout the value chain.\\n\\nAI Office\\xa0may invite GPAI model providers, relevant national competent authorities to participate in drawing up the codes, while civil society, industry, academia, downstream providers and independent experts may support the process.\\n\\nGovernance\\n\\nHow will the AI Act be implemented?\\n\\nThe\\xa0AI Office\\xa0will be established, sitting within the Commission, to monitor the effective implementation and compliance of GPAI model providers.\\n\\nDownstream providers can lodge a complaint regarding the upstream providers infringement to the AI Office.\\n\\nThe AI Office may conduct evaluations of the GPAI model to:\\n\\nassess compliance where the information gathered under its powers to request information is insufficient.\\n\\nInvestigate systemic risks, particularly following a qualified report from the scientific panel of independent experts.\\n\\nTimelines\\n\\nAfter entry into force, the AI Act will apply by the following deadlines:\\n\\n6 months for prohibited AI systems.\\n\\n12 months for GPAI.\\xa0\\n\\n24 months for high risk AI systems under Annex III.\\xa0\\n\\n36 months for high risk AI systems under Annex I.')]\n"
     ]
    }
   ],
   "source": [
    "print(text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75182f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for extracted images\n",
    "\n",
    "for doc in text_docs:\n",
    "    if 'images' in doc.metadata:\n",
    "        images = doc.metadata['images']\n",
    "        print(f\"Extracted {len(images)} images on this page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b865480e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='High-level summary of the AI Act\\n\\n27 Feb, 2024\\n\\nUpdated on 30 May in accordance with the Corrigendum version of the AI Act.\\n\\nIn this article we provide you with a high-level summary of the AI Act, selecting the parts which are most likely to be relevant to you regardless of who you are. We provide links to the original document where relevant so that you can always reference the Act text.\\n\\nTo explore the full text of the AI Act yourself, use our\\xa0AI Act Explorer. Alternatively, if you want to know which parts of the text are most relevant to you, use our\\xa0Compliance Checker. \\n\\nFour-point summary\\n\\nThe AI Act classifies AI according to its risk:\\n\\nUnacceptable risk is prohibited (e.g. social scoring systems and manipulative AI).\\n\\nMost of the text addresses high-risk AI systems, which are regulated.'),\n",
       " Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='Unacceptable risk is prohibited (e.g. social scoring systems and manipulative AI).\\n\\nMost of the text addresses high-risk AI systems, which are regulated.\\n\\nA smaller section handles limited risk AI systems, subject to lighter transparency obligations: developers and deployers must ensure that end-users are aware that they are interacting with AI (chatbots and deepfakes).\\n\\nMinimal risk is unregulated (including the majority of AI applications currently available on the EU single market, such as AI enabled video games and spam filters – at least in 2021; this is changing with generative AI).\\n\\nThe majority of obligations fall on providers (developers) of high-risk AI systems.\\n\\nThose that intend to place on the market or put into service high-risk AI systems in the EU, regardless of whether they are based in the EU or a third country.\\n\\nAnd also third country providers where the high risk AI system’s output is used in the EU.'),\n",
       " Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='And also third country providers where the high risk AI system’s output is used in the EU.\\n\\nUsers are natural or legal persons that deploy an AI system in a professional capacity, not affected end-users.\\n\\nUsers (deployers) of high-risk AI systems have some obligations, though less than providers (developers).\\n\\nThis applies to users located in the EU, and third country users where the AI system’s output is used in the EU.\\n\\nGeneral purpose AI (GPAI):\\n\\nAll GPAI model providers must provide technical documentation, instructions for use, comply with the Copyright Directive, and publish a summary about the content used for training.\\n\\nFree and open licence GPAI model providers only need to comply with copyright and publish the training data summary, unless they present a systemic risk.\\n\\nAll providers of GPAI models that present a systemic risk – open or closed – must also conduct model evaluations, adversarial testing, track and report serious incidents and ensure cybersecurity protections.'),\n",
       " Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='Prohibited AI systems (Chapter II,\\xa0Art. 5)\\n\\nThe following types of AI system are ‘Prohibited’ according to the AI Act.\\n\\nAI systems:\\n\\ndeploying\\xa0subliminal, manipulative, or deceptive techniques\\xa0to distort behaviour and impair informed decision-making, causing significant harm.\\n\\nexploiting vulnerabilities\\xa0related to age, disability, or socio-economic circumstances to distort behaviour, causing significant harm.\\n\\nbiometric categorisation systems\\xa0inferring sensitive attributes (race, political opinions, trade union membership, religious or philosophical beliefs, sex life, or sexual orientation), except labelling or filtering of lawfully acquired biometric datasets or when law enforcement categorises biometric data.\\n\\nsocial scoring, i.e., evaluating or classifying individuals or groups based on social behaviour or personal traits, causing detrimental or unfavourable treatment of those people.'),\n",
       " Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='social scoring, i.e., evaluating or classifying individuals or groups based on social behaviour or personal traits, causing detrimental or unfavourable treatment of those people.\\n\\nassessing the risk of an individual committing criminal offenses\\xa0solely based on profiling or personality traits, except when used to augment human assessments based on objective, verifiable facts directly linked to criminal activity.\\n\\ncompiling facial recognition databases\\xa0by untargeted scraping of facial images from the internet or CCTV footage.\\n\\ninferring emotions in workplaces or educational institutions, except for medical or safety reasons.\\n\\n‘real-time’ remote biometric identification (RBI) in publicly accessible spaces for law enforcement, except when:\\n\\nsearching for missing persons, abduction victims, and people who have been human trafficked or sexually exploited;\\n\\npreventing substantial and imminent threat to life, or foreseeable terrorist attack; or')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Splittings \n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "docs = text_splitter.split_documents(text_docs)\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e9c2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Embeddings and Vectorstore\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(docs, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b875cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-level summary of the AI Act\n",
      "\n",
      "27 Feb, 2024\n",
      "\n",
      "Updated on 30 May in accordance with the Corrigendum version of the AI Act.\n",
      "\n",
      "In this article we provide you with a high-level summary of the AI Act, selecting the parts which are most likely to be relevant to you regardless of who you are. We provide links to the original document where relevant so that you can always reference the Act text.\n",
      "\n",
      "To explore the full text of the AI Act yourself, use our AI Act Explorer. Alternatively, if you want to know which parts of the text are most relevant to you, use our Compliance Checker. \n",
      "\n",
      "Four-point summary\n",
      "\n",
      "The AI Act classifies AI according to its risk:\n",
      "\n",
      "Unacceptable risk is prohibited (e.g. social scoring systems and manipulative AI).\n",
      "\n",
      "Most of the text addresses high-risk AI systems, which are regulated.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the four point summary?\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaecc902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x00000298575A5C10> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000298575A6B10> root_client=<openai.OpenAI object at 0x00000298575A58D0> root_async_client=<openai.AsyncOpenAI object at 0x00000298575A6650> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b849ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            Answer the following question based only on the context. \n",
    "            Think step by step defore providing a detailed answer. \n",
    "            I will tip you $1000 if the used finds the answer helpful.\n",
    "            Context : {context}\n",
    "            Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49e58351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000029856EB73D0>, search_kwargs={})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96cf01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b1610d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "198d84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = retrieval_chain.invoke({\"input\":\"An attention function can be described as mapping a query\"})\n",
    "response = retrieval_chain.invoke({\"input\":\"What is the four point summary?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fd03121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the four point summary?',\n",
       " 'context': [Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='High-level summary of the AI Act\\n\\n27 Feb, 2024\\n\\nUpdated on 30 May in accordance with the Corrigendum version of the AI Act.\\n\\nIn this article we provide you with a high-level summary of the AI Act, selecting the parts which are most likely to be relevant to you regardless of who you are. We provide links to the original document where relevant so that you can always reference the Act text.\\n\\nTo explore the full text of the AI Act yourself, use our\\xa0AI Act Explorer. Alternatively, if you want to know which parts of the text are most relevant to you, use our\\xa0Compliance Checker. \\n\\nFour-point summary\\n\\nThe AI Act classifies AI according to its risk:\\n\\nUnacceptable risk is prohibited (e.g. social scoring systems and manipulative AI).\\n\\nMost of the text addresses high-risk AI systems, which are regulated.'),\n",
       "  Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='GPAI models present systemic risks when the cumulative amount of compute used for its training is greater than 1025\\xa0floating point operations (FLOPs). Providers must notify the Commission if their model meets this criterion within 2 weeks. The provider may present arguments that, despite meeting the criteria, their model does not present systemic risks. The Commission may decide on its own, or via a qualified alert from the scientific panel of independent experts, that a model has high impact capabilities, rendering it systemic.\\n\\nIn addition to the four obligations above, providers of GPAI models with systemic risk must also:\\n\\nPerform\\xa0model evaluations, including conducting and documenting\\xa0adversarial testing\\xa0to identify and mitigate systemic risk.\\n\\nAssess and mitigate possible systemic risks, including their sources.\\n\\nTrack, document and report serious incidents\\xa0and possible corrective measures to the\\xa0AI Office\\xa0and relevant national competent authorities without undue delay.'),\n",
       "  Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='High-level summary of the AI Act\\n\\n27 Feb, 2024\\n\\nUpdated on 30 May in accordance with the Corrigendum version of the AI Act.\\n\\nIn this article we provide you with a high-level summary of the AI Act, selecting the parts which are most likely to be relevant to you regardless of who you are. We provide links to the original document where relevant so that you can always reference the Act text.\\n\\nTo explore the full text of the AI Act yourself, use our\\xa0AI Act Explorer. Alternatively, if you want to know which parts of the text are most relevant to you, use our\\xa0Compliance Checker. \\n\\nFour-point summary\\n\\nThe AI Act classifies AI according to its risk:\\n\\nUnacceptable risk is prohibited (e.g. social scoring systems and manipulative AI).\\n\\nMost of the text addresses high-risk AI systems, which are regulated.\\n\\nA smaller section handles limited risk AI systems, subject to lighter transparency obligations: developers and deployers must ensure that end-users are aware that they are interacting with AI (chatbots and deepfakes).\\n\\nMinimal risk is unregulated (including the majority of AI applications currently available on the EU single market, such as AI enabled video games and spam filters – at least in 2021; this is changing with generative AI).\\n\\nThe majority of obligations fall on providers (developers) of high-risk AI systems.\\n\\nThose that intend to place on the market or put into service high-risk AI systems in the EU, regardless of whether they are based in the EU or a third country.\\n\\nAnd also third country providers where the high risk AI system’s output is used in the EU.\\n\\nUsers are natural or legal persons that deploy an AI system in a professional capacity, not affected end-users.\\n\\nUsers (deployers) of high-risk AI systems have some obligations, though less than providers (developers).\\n\\nThis applies to users located in the EU, and third country users where the AI system’s output is used in the EU.\\n\\nGeneral purpose AI (GPAI):\\n\\nAll GPAI model providers must provide technical documentation, instructions for use, comply with the Copyright Directive, and publish a summary about the content used for training.\\n\\nFree and open licence GPAI model providers only need to comply with copyright and publish the training data summary, unless they present a systemic risk.\\n\\nAll providers of GPAI models that present a systemic risk – open or closed – must also conduct model evaluations, adversarial testing, track and report serious incidents and ensure cybersecurity protections.\\n\\nProhibited AI systems (Chapter II,\\xa0Art. 5)\\n\\nThe following types of AI system are ‘Prohibited’ according to the AI Act.\\n\\nAI systems:\\n\\ndeploying\\xa0subliminal, manipulative, or deceptive techniques\\xa0to distort behaviour and impair informed decision-making, causing significant harm.\\n\\nexploiting vulnerabilities\\xa0related to age, disability, or socio-economic circumstances to distort behaviour, causing significant harm.\\n\\nbiometric categorisation systems\\xa0inferring sensitive attributes (race, political opinions, trade union membership, religious or philosophical beliefs, sex life, or sexual orientation), except labelling or filtering of lawfully acquired biometric datasets or when law enforcement categorises biometric data.\\n\\nsocial scoring, i.e., evaluating or classifying individuals or groups based on social behaviour or personal traits, causing detrimental or unfavourable treatment of those people.\\n\\nassessing the risk of an individual committing criminal offenses\\xa0solely based on profiling or personality traits, except when used to augment human assessments based on objective, verifiable facts directly linked to criminal activity.\\n\\ncompiling facial recognition databases\\xa0by untargeted scraping of facial images from the internet or CCTV footage.\\n\\ninferring emotions in workplaces or educational institutions, except for medical or safety reasons.\\n\\n‘real-time’ remote biometric identification (RBI) in publicly accessible spaces for law enforcement, except when:\\n\\nsearching for missing persons, abduction victims, and people who have been human trafficked or sexually exploited;\\n\\npreventing substantial and imminent threat to life, or foreseeable terrorist attack; or\\n\\nidentifying suspects in serious crimes (e.g., murder, rape, armed robbery, narcotic and illegal weapons trafficking, organised crime, and environmental crime, etc.).\\n\\nNotes on remote biometric identification:\\n\\nUsing AI-enabled real-time RBI is only allowed\\xa0when not using the tool would cause considerable harm\\xa0and must account for affected persons’ rights and freedoms.\\n\\nBefore deployment, police must complete a\\xa0fundamental rights impact assessment\\xa0and\\xa0register the system in the EU database, though, in duly justified cases of urgency, deployment can commence without registration, provided that it is registered later without undue delay.\\n\\nBefore deployment, they also must obtain\\xa0authorisation from a judicial authority or independent administrative authority[1], though, in duly justified cases of urgency, deployment can commence without authorisation, provided that authorisation is requested within 24 hours. If authorisation is rejected, deployment must cease immediately, deleting all data, results, and outputs.\\n\\n↲\\xa0[1] Independent administrative authorities may be subject to greater political influence than judicial authorities (Hacker, 2024).\\n\\nHigh risk AI systems (Chapter III)\\n\\nSome AI systems are considered ‘High risk’ under the AI Act. Providers of those systems will be subject to additional requirements.\\n\\nClassification rules for high-risk AI systems (Art. 6)\\n\\nHigh risk AI systems are those:\\n\\nused as a safety component or a product covered by EU laws in\\xa0Annex I\\xa0AND\\xa0required to undergo a third-party conformity assessment under those\\xa0Annex I\\xa0laws;\\xa0OR\\n\\nthose under\\xa0Annex III\\xa0use cases (below), except if:\\n\\nthe AI system performs a narrow procedural task;\\n\\nimproves the result of a previously completed human activity;\\n\\ndetects decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment without proper human review; or\\n\\nperforms a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III.\\n\\nAI systems are always considered high-risk if it profiles individuals, i.e. automated processing of personal\\ndata to assess various aspects of a person’s life, such as work performance, economic situation, health,\\npreferences, interests, reliability, behaviour, location or movement.\\n\\nProviders whose AI system falls under the use cases in\\xa0Annex III\\xa0but believes it is\\xa0not\\xa0high-risk must document such an\\nassessment before placing it on the market or putting it into service.\\n\\nRequirements for providers of high-risk AI systems (Art.\\xa08–17)\\n\\nHigh risk AI providers must:\\n\\nEstablish a\\xa0risk management system\\xa0throughout the high risk AI system’s lifecycle;\\n\\nConduct\\xa0data governance, ensuring that training, validation and testing datasets are relevant, sufficiently representative and, to the best extent possible, free of errors and complete according to the intended purpose.\\n\\nDraw up\\xa0technical documentation\\xa0to demonstrate compliance and provide authorities with the information to assess that compliance.\\n\\nDesign their high risk AI system for\\xa0record-keeping\\xa0to enable it to automatically record events relevant for identifying national level risks and substantial modifications throughout the system’s lifecycle.\\n\\nProvide\\xa0instructions for use\\xa0to downstream deployers to enable the latter’s compliance.\\n\\nDesign their high risk AI system to allow deployers to implement\\xa0human oversight.\\n\\nDesign their high risk AI system to achieve appropriate levels of\\xa0accuracy, robustness, and cybersecurity.\\n\\nEstablish a\\xa0quality management system\\xa0to ensure compliance.\\n\\nAnnex III\\xa0use cases\\n\\nNon-banned biometrics:\\xa0Remote biometric identification systems, excluding biometric verification that confirm a person is who they claim to be. Biometric categorisation systems inferring sensitive or protected attributes or characteristics. Emotion recognition systems.\\n\\nCritical infrastructure:\\xa0Safety components in the management and operation of critical digital infrastructure, road traffic and the supply of water, gas, heating and electricity.\\n\\nEducation and vocational training:\\xa0AI systems determining access, admission or assignment to educational and vocational training institutions at all levels. Evaluating learning outcomes, including those used to steer the student’s learning process. Assessing the appropriate level of education for an individual. Monitoring and detecting prohibited student behaviour during tests.\\n\\nEmployment, workers management and access to self-employment:\\xa0AI systems used for recruitment or selection, particularly targeted job ads, analysing and filtering applications, and evaluating candidates. Promotion and termination of contracts, allocating tasks based on personality traits or characteristics and behaviour, and monitoring and evaluating performance.\\n\\nAccess to and enjoyment of essential public and private services:\\xa0AI systems used by public authorities for assessing eligibility to benefits and services, including their allocation, reduction, revocation, or recovery. Evaluating creditworthiness, except when detecting financial fraud. Evaluating and classifying emergency calls, including dispatch prioritising of police, firefighters, medical aid and urgent patient triage services. Risk assessments and pricing in health and life insurance.\\n\\nLaw enforcement:\\xa0\\xa0AI systems used to assess an individual’s risk of becoming a crime victim. Polygraphs. Evaluating evidence reliability during criminal investigations or prosecutions. Assessing an individual’s risk of offending or re-offending not solely based on profiling or assessing personality traits or past criminal behaviour. Profiling during criminal detections, investigations or prosecutions.'),\n",
       "  Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='All providers of GPAI models must:\\n\\nDraw up\\xa0technical documentation, including training and testing process and evaluation results.\\n\\nDraw up\\xa0information and documentation to supply to downstream providers\\xa0that intend to integrate the GPAI model into their own AI system in order that the latter understands capabilities and limitations and is enabled to comply.\\n\\nEstablish a policy to\\xa0respect the Copyright Directive.\\n\\nPublish a\\xa0sufficiently detailed summary about the content used for training\\xa0the GPAI model.\\n\\nFree and open licence GPAI models\\xa0– whose parameters, including weights, model architecture and model usage are publicly available, allowing for access, usage, modification and distribution of the model – only have to comply with the latter two obligations above, unless the free and open licence GPAI model is systemic.')],\n",
       " 'answer': \"To answer your question step by step, let's focus on the four-point summary of the AI Act as provided in the context. Here's a breakdown:\\n\\n1. **AI Classification by Risk**: \\n   - **Unacceptable Risk**: AI systems that carry an unacceptable risk are prohibited. Examples include social scoring systems and manipulative AI.\\n   - **High-Risk Systems**: The majority of the Act focuses on high-risk AI systems, which are subject to regulation.\\n\\n2. **GPAI Models and Systemic Risks**:\\n   - GPAI (General Purpose AI) models are considered systemic risks if they use an extensive amount of computational power (more than \\\\(10^{25}\\\\) floating point operations). Providers must notify the Commission within two weeks if this criterion is met.\\n   - Providers can argue that despite meeting the criteria, their model doesn't pose a systemic risk. The Commission, with advice from a scientific panel, may determine if a model has significant impact capabilities that qualify it as systemic.\\n\\n3. **Obligations for Systemic GPAI Models**:\\n   - Providers with systemic-risk GPAI models must perform model evaluations, conduct adversarial testing, document systemic risks and their sources, and track and report serious incidents to relevant authorities promptly.\\n\\n4. **Risk-Based Regulation and Obligations**:\\n   - Obligations are mainly on providers (developers) of high-risk AI systems and those outside the EU whose output is used within the EU.\\n   - Users (deployers) of high-risk AI systems have fewer obligations compared to providers.\\n\\nThis summary gives an overall understanding of the AI Act's focus, categorizing AI systems by risk and detailing the responsibilities and regulations for each risk level, with particular attention to GPAI models and high-risk systems.\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "085288df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer your question step by step, let's focus on the four-point summary of the AI Act as provided in the context. Here's a breakdown:\n",
      "\n",
      "1. **AI Classification by Risk**: \n",
      "   - **Unacceptable Risk**: AI systems that carry an unacceptable risk are prohibited. Examples include social scoring systems and manipulative AI.\n",
      "   - **High-Risk Systems**: The majority of the Act focuses on high-risk AI systems, which are subject to regulation.\n",
      "\n",
      "2. **GPAI Models and Systemic Risks**:\n",
      "   - GPAI (General Purpose AI) models are considered systemic risks if they use an extensive amount of computational power (more than \\(10^{25}\\) floating point operations). Providers must notify the Commission within two weeks if this criterion is met.\n",
      "   - Providers can argue that despite meeting the criteria, their model doesn't pose a systemic risk. The Commission, with advice from a scientific panel, may determine if a model has significant impact capabilities that qualify it as systemic.\n",
      "\n",
      "3. **Obligations for Systemic GPAI Models**:\n",
      "   - Providers with systemic-risk GPAI models must perform model evaluations, conduct adversarial testing, document systemic risks and their sources, and track and report serious incidents to relevant authorities promptly.\n",
      "\n",
      "4. **Risk-Based Regulation and Obligations**:\n",
      "   - Obligations are mainly on providers (developers) of high-risk AI systems and those outside the EU whose output is used within the EU.\n",
      "   - Users (deployers) of high-risk AI systems have fewer obligations compared to providers.\n",
      "\n",
      "This summary gives an overall understanding of the AI Act's focus, categorizing AI systems by risk and detailing the responsibilities and regulations for each risk level, with particular attention to GPAI models and high-risk systems.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24889fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unacceptable risk is prohibited (e.g. social scoring systems and manipulative AI).\n",
      "\n",
      "Most of the text addresses high-risk AI systems, which are regulated.\n",
      "\n",
      "A smaller section handles limited risk AI systems, subject to lighter transparency obligations: developers and deployers must ensure that end-users are aware that they are interacting with AI (chatbots and deepfakes).\n",
      "\n",
      "Minimal risk is unregulated (including the majority of AI applications currently available on the EU single market, such as AI enabled video games and spam filters – at least in 2021; this is changing with generative AI).\n",
      "\n",
      "The majority of obligations fall on providers (developers) of high-risk AI systems.\n",
      "\n",
      "Those that intend to place on the market or put into service high-risk AI systems in the EU, regardless of whether they are based in the EU or a third country.\n",
      "\n",
      "And also third country providers where the high risk AI system’s output is used in the EU.\n"
     ]
    }
   ],
   "source": [
    "# query = \"What is multi head attention?\"\n",
    "\n",
    "query = \"Requirements for providers of high-risk AI systems\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "671f5268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='Providers whose AI system falls under the use cases in\\xa0Annex III\\xa0but believes it is\\xa0not\\xa0high-risk must document such an\\nassessment before placing it on the market or putting it into service.\\n\\nRequirements for providers of high-risk AI systems (Art.\\xa08–17)\\n\\nHigh risk AI providers must:\\n\\nEstablish a\\xa0risk management system\\xa0throughout the high risk AI system’s lifecycle;\\n\\nConduct\\xa0data governance, ensuring that training, validation and testing datasets are relevant, sufficiently representative and, to the best extent possible, free of errors and complete according to the intended purpose.\\n\\nDraw up\\xa0technical documentation\\xa0to demonstrate compliance and provide authorities with the information to assess that compliance.\\n\\nDesign their high risk AI system for\\xa0record-keeping\\xa0to enable it to automatically record events relevant for identifying national level risks and substantial modifications throughout the system’s lifecycle.'),\n",
       " Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='Unacceptable risk is prohibited (e.g. social scoring systems and manipulative AI).\\n\\nMost of the text addresses high-risk AI systems, which are regulated.\\n\\nA smaller section handles limited risk AI systems, subject to lighter transparency obligations: developers and deployers must ensure that end-users are aware that they are interacting with AI (chatbots and deepfakes).\\n\\nMinimal risk is unregulated (including the majority of AI applications currently available on the EU single market, such as AI enabled video games and spam filters – at least in 2021; this is changing with generative AI).\\n\\nThe majority of obligations fall on providers (developers) of high-risk AI systems.\\n\\nThose that intend to place on the market or put into service high-risk AI systems in the EU, regardless of whether they are based in the EU or a third country.\\n\\nAnd also third country providers where the high risk AI system’s output is used in the EU.'),\n",
       " Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='And also third country providers where the high risk AI system’s output is used in the EU.\\n\\nUsers are natural or legal persons that deploy an AI system in a professional capacity, not affected end-users.\\n\\nUsers (deployers) of high-risk AI systems have some obligations, though less than providers (developers).\\n\\nThis applies to users located in the EU, and third country users where the AI system’s output is used in the EU.\\n\\nGeneral purpose AI (GPAI):\\n\\nAll GPAI model providers must provide technical documentation, instructions for use, comply with the Copyright Directive, and publish a summary about the content used for training.\\n\\nFree and open licence GPAI model providers only need to comply with copyright and publish the training data summary, unless they present a systemic risk.\\n\\nAll providers of GPAI models that present a systemic risk – open or closed – must also conduct model evaluations, adversarial testing, track and report serious incidents and ensure cybersecurity protections.'),\n",
       " Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx'}, page_content='Before deployment, they also must obtain\\xa0authorisation from a judicial authority or independent administrative authority[1], though, in duly justified cases of urgency, deployment can commence without authorisation, provided that authorisation is requested within 24 hours. If authorisation is rejected, deployment must cease immediately, deleting all data, results, and outputs.\\n\\n↲\\xa0[1] Independent administrative authorities may be subject to greater political influence than judicial authorities (Hacker, 2024).\\n\\nHigh risk AI systems (Chapter III)\\n\\nSome AI systems are considered ‘High risk’ under the AI Act. Providers of those systems will be subject to additional requirements.\\n\\nClassification rules for high-risk AI systems (Art. 6)\\n\\nHigh risk AI systems are those:\\n\\nused as a safety component or a product covered by EU laws in\\xa0Annex I\\xa0AND\\xa0required to undergo a third-party conformity assessment under those\\xa0Annex I\\xa0laws;\\xa0OR\\n\\nthose under\\xa0Annex III\\xa0use cases (below), except if:')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retireved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c155243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceadar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
