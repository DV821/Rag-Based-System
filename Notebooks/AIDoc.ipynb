{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5d7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9925dc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\Ceadar Challenge\\ceadar\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion \n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "loader = PyMuPDFLoader('../Data/EU AI Act Doc (1) (3).docx',\n",
    "                       extract_images = True,\n",
    "                    #    mode = 'single'\n",
    "                       )\n",
    "text_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86acee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='High-level summary of the AI Act\\n27 Feb, 2024\\nUpdated on 30 May in accordance with the Corrigendum version of\\nthe AI Act.\\nIn this article we provide you with a high-level summary of the AI\\nAct, selecting the parts which are most likely to be relevant to you\\nregardless of who you are. We provide links to the original\\ndocument where relevant so that you can always reference the Act\\ntext.\\nTo explore the full text of the AI Act yourself, use our\\xa0AI Act\\nExplorer. Alternatively, if you want to know which parts of the text\\nare most relevant to you, use our\\xa0Compliance Checker.\\nFour-point summary\\nThe AI Act classifies AI according to its risk:\\nUnacceptable risk is prohibited (e.g. social scoring systems and\\nmanipulative AI).\\nMost of the text addresses high-risk AI systems, which are\\nregulated.\\nA smaller section handles limited risk AI systems, subject to lighter\\ntransparency obligations: developers and deployers must ensure\\nthat end-users are aware that they are interacting with AI (chatbots\\nand deepfakes).\\nMinimal risk is unregulated (including the majority of AI\\napplications currently available on the EU single market, such as AI\\nenabled video games and spam filters – at least in 2021; this is\\nchanging with generative AI).\\nThe majority of obligations fall on providers (developers) of high-\\nrisk AI systems.\\nThose that intend to place on the market or put into service high-'), Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='risk AI systems in the EU, regardless of whether they are based in\\nthe EU or a third country.\\nAnd also third country providers where the high risk AI system’s\\noutput is used in the EU.\\nUsers are natural or legal persons that deploy an AI system in a\\nprofessional capacity, not affected end-users.\\nUsers (deployers) of high-risk AI systems have some obligations,\\nthough less than providers (developers).\\nThis applies to users located in the EU, and third country users\\nwhere the AI system’s output is used in the EU.\\nGeneral purpose AI (GPAI):\\nAll GPAI model providers must provide technical documentation,\\ninstructions for use, comply with the Copyright Directive, and\\npublish a summary about the content used for training.\\nFree and open licence GPAI model providers only need to comply\\nwith copyright and publish the training data summary, unless they\\npresent a systemic risk.\\nAll providers of GPAI models that present a systemic risk – open or\\nclosed – must also conduct model evaluations, adversarial testing,\\ntrack and report serious incidents and ensure cybersecurity\\nprotections.\\nProhibited AI systems (Chapter II,\\xa0Art. 5)\\nThe following types of AI system are ‘Prohibited’ according to the\\nAI Act.\\nAI systems:\\ndeploying\\xa0subliminal, manipulative, or deceptive techniques\\xa0to\\ndistort behaviour and impair informed decision-making, causing\\nsignificant harm.'), Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='exploiting vulnerabilities\\xa0related to age, disability, or socio-\\neconomic circumstances to distort behaviour, causing significant\\nharm.\\nbiometric categorisation systems\\xa0inferring sensitive attributes (race,\\npolitical opinions, trade union membership, religious or\\nphilosophical beliefs, sex life, or sexual orientation), except\\nlabelling or filtering of lawfully acquired biometric datasets or\\nwhen law enforcement categorises biometric data.\\nsocial scoring, i.e., evaluating or classifying individuals or groups\\nbased on social behaviour or personal traits, causing detrimental or\\nunfavourable treatment of those people.\\nassessing the risk of an individual committing criminal\\noffenses\\xa0solely based on profiling or personality traits, except when\\nused to augment human assessments based on objective, verifiable\\nfacts directly linked to criminal activity.\\ncompiling facial recognition databases\\xa0by untargeted scraping of\\nfacial images from the internet or CCTV footage.\\ninferring emotions in workplaces or educational institutions, except\\nfor medical or safety reasons.\\n‘real-time’ remote biometric identification (RBI) in publicly\\naccessible spaces for law enforcement, except when:\\nsearching for missing persons, abduction victims, and people who\\nhave been human trafficked or sexually exploited;\\npreventing substantial and imminent threat to life, or foreseeable\\nterrorist attack; or\\nidentifying suspects in serious crimes (e.g., murder, rape, armed\\nrobbery, narcotic and illegal weapons trafficking, organised crime,\\nand environmental crime, etc.).\\nNotes on remote biometric identification:\\nUsing AI-enabled real-time RBI is only allowed\\xa0when not using the'), Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='tool would cause considerable harm\\xa0and must account for affected\\npersons’ rights and freedoms.\\nBefore deployment, police must complete a\\xa0fundamental rights\\nimpact assessment\\xa0and\\xa0register the system in the EU database,\\nthough, in duly justified cases of urgency, deployment can\\ncommence without registration, provided that it is registered later\\nwithout undue delay.\\nBefore deployment, they also must obtain\\xa0authorisation from a\\njudicial authority or independent administrative authority[1],\\nthough, in duly justified cases of urgency, deployment can\\ncommence without authorisation, provided that authorisation is\\nrequested within 24 hours. If authorisation is rejected, deployment\\nmust cease immediately, deleting all data, results, and outputs.\\n↲\\xa0[1] Independent administrative authorities may be subject to\\ngreater political influence than judicial authorities (Hacker, 2024).\\nHigh risk AI systems (Chapter III)\\nSome AI systems are considered ‘High risk’ under the AI Act.\\nProviders of those systems will be subject to additional\\nrequirements.\\nClassification rules for high-risk AI systems (Art. 6)\\nHigh risk AI systems are those:\\nused as a safety component or a product covered by EU laws\\nin\\xa0Annex I\\xa0AND\\xa0required to undergo a third-party conformity\\nassessment under those\\xa0Annex I\\xa0laws;\\xa0OR\\nthose under\\xa0Annex III\\xa0use cases (below), except if:\\nthe AI system performs a narrow procedural task;\\nimproves the result of a previously completed human activity;\\ndetects decision-making patterns or deviations from prior decision-\\nmaking patterns and is not meant to replace or influence the'), Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='previously completed human assessment without proper human\\nreview; or\\nperforms a preparatory task to an assessment relevant for the\\npurpose of the use cases listed in Annex III.\\nAI systems are always considered high-risk if it profiles individuals,\\ni.e. automated processing of personal data to assess various aspects\\nof a person’s life, such as work performance, economic situation,\\nhealth, preferences, interests, reliability, behaviour, location or\\nmovement.\\nProviders whose AI system falls under the use cases in\\xa0Annex III\\xa0but\\nbelieves it is\\xa0not\\xa0high-risk must document such an assessment\\nbefore placing it on the market or putting it into service.\\nRequirements for providers of high-risk AI systems (Art.\\xa08–17)\\nHigh risk AI providers must:\\nEstablish a\\xa0risk management system\\xa0throughout the high risk AI\\nsystem’s lifecycle;\\nConduct\\xa0data governance, ensuring that training, validation and\\ntesting datasets are relevant, sufficiently representative and, to the\\nbest extent possible, free of errors and complete according to the\\nintended purpose.\\nDraw up\\xa0technical documentation\\xa0to demonstrate compliance and\\nprovide authorities with the information to assess that compliance.\\nDesign their high risk AI system for\\xa0record-keeping\\xa0to enable it to\\nautomatically record events relevant for identifying national level\\nrisks and substantial modifications throughout the system’s\\nlifecycle.\\nProvide\\xa0instructions for use\\xa0to downstream deployers to enable the\\nlatter’s compliance.\\nDesign their high risk AI system to allow deployers to\\nimplement\\xa0human oversight.'), Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Design their high risk AI system to achieve appropriate levels\\nof\\xa0accuracy, robustness, and cybersecurity.\\nEstablish a\\xa0quality management system\\xa0to ensure compliance.\\nAnnex III\\xa0use cases\\nNon-banned biometrics:\\xa0Remote biometric identification systems,\\nexcluding biometric verification that confirm a person is who they\\nclaim to be. Biometric categorisation systems inferring sensitive or\\nprotected attributes or characteristics. Emotion recognition systems.\\nCritical infrastructure:\\xa0Safety components in the management and\\noperation of critical digital infrastructure, road traffic and the\\nsupply of water, gas, heating and electricity.\\nEducation and vocational training:\\xa0AI systems determining access,\\nadmission or assignment to educational and vocational training\\ninstitutions at all levels. Evaluating learning outcomes, including\\nthose used to steer the student’s learning process. Assessing the\\nappropriate level of education for an individual. Monitoring and\\ndetecting prohibited student behaviour during tests.\\nEmployment, workers management and access to self-\\nemployment:\\xa0AI systems used for recruitment or selection,\\nparticularly targeted job ads, analysing and filtering applications,\\nand evaluating candidates. Promotion and termination of contracts,\\nallocating tasks based on personality traits or characteristics and\\nbehaviour, and monitoring and evaluating performance.\\nAccess to and enjoyment of essential public and private services:\\xa0AI\\nsystems used by public authorities for assessing eligibility to\\nbenefits and services, including their allocation, reduction,\\nrevocation, or recovery. Evaluating creditworthiness, except when\\ndetecting financial fraud. Evaluating and classifying emergency\\ncalls, including dispatch prioritising of police, firefighters, medical\\naid and urgent patient triage services. Risk assessments and pricing\\nin health and life insurance.\\nLaw enforcement:\\xa0\\xa0AI systems used to assess an individual’s risk of'), Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='becoming a crime victim. Polygraphs. Evaluating evidence\\nreliability during criminal investigations or prosecutions. Assessing\\nan individual’s risk of offending or re-offending not solely based on\\nprofiling or assessing personality traits or past criminal behaviour.\\nProfiling during criminal detections, investigations or prosecutions.\\nMigration, asylum and border control management:\\xa0\\xa0Polygraphs.\\nAssessments of irregular migration or health risks. Examination of\\napplications for asylum, visa and residence permits, and associated\\ncomplaints related to eligibility. Detecting, recognising or\\nidentifying individuals, except verifying travel documents.\\nAdministration of justice and democratic processes:\\xa0\\xa0AI systems\\nused in researching and interpreting facts and applying the law to\\nconcrete facts or used in alternative dispute resolution. Influencing\\nelections and referenda outcomes or voting behaviour, excluding\\noutputs that do not directly interact with people, like tools used to\\norganise, optimise and structure political campaigns.\\nGeneral purpose AI (GPAI)\\nGPAI model\\xa0means an AI model, including when trained with a\\nlarge amount of data using self-supervision at scale, that displays\\nsignificant generality and is capable to competently perform a wide\\nrange of distinct tasks regardless of the way the model is placed on\\nthe market and that can be integrated into a variety of downstream\\nsystems or applications. This\\ndoes not cover AI models that are used before release on the market\\nfor research, development and prototyping activities.\\nGPAI system\\xa0means an AI system which is based on a general\\npurpose AI model, that has the capability to serve a variety of\\npurposes, both for direct use as well as for integration in other AI\\nsystems.\\nGPAI systems may be used as high risk AI systems or integrated into\\nthem. GPAI system providers should cooperate with such high risk\\nAI system providers to enable the latter’s compliance.\\nAll providers of GPAI models must:'), Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Draw up\\xa0technical documentation, including training and testing\\nprocess and evaluation results.\\nDraw up\\xa0information and documentation to supply to downstream\\nproviders\\xa0that intend to integrate the GPAI model into their own AI\\nsystem in order that the latter understands capabilities and\\nlimitations and is enabled to comply.\\nEstablish a policy to\\xa0respect the Copyright Directive.\\nPublish a\\xa0sufficiently detailed summary about the content used for\\ntraining\\xa0the GPAI model.\\nFree and open licence GPAI models\\xa0– whose parameters, including\\nweights, model architecture and model usage are publicly available,\\nallowing for access, usage, modification and distribution of the\\nmodel – only have to comply with the latter two obligations above,\\nunless the free and open licence GPAI model is systemic.\\nGPAI models present systemic risks when the cumulative amount of\\ncompute used for its training is greater than 1025\\xa0floating point\\noperations (FLOPs). Providers must notify the Commission if their\\nmodel meets this criterion within 2 weeks. The provider may\\npresent arguments that, despite meeting the criteria, their model\\ndoes not present systemic risks. The Commission may decide on its\\nown, or via a qualified alert from the scientific panel of\\nindependent experts, that a model has high impact capabilities,\\nrendering it systemic.\\nIn addition to the four obligations above, providers of GPAI models\\nwith systemic risk must also:\\nPerform\\xa0model evaluations, including conducting and\\ndocumenting\\xa0adversarial testing\\xa0to identify and mitigate systemic\\nrisk.\\nAssess and mitigate possible systemic risks, including their sources.\\nTrack, document and report serious incidents\\xa0and possible\\ncorrective measures to the\\xa0AI Office\\xa0and relevant national\\ncompetent authorities without undue delay.'), Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Ensure an adequate level of\\xa0cybersecurity protection.\\nAll GPAI model providers may demonstrate compliance with their\\nobligations if they voluntarily adhere to a code of practice until\\nEuropean harmonised standards are published, compliance with\\nwhich will lead to a presumption of conformity. Providers that don’t\\nadhere to codes of practice must demonstrate\\xa0alternative adequate\\nmeans of\\xa0compliance\\xa0for Commission approval.\\nCodes of practice\\nWill account for international approaches.\\nWill cover but not necessarily limited to the above obligations,\\nparticularly the relevant information to include in technical\\ndocumentation for authorities and downstream providers,\\nidentification of the type and nature of systemic risks and their\\nsources, and the modalities of risk management accounting for\\nspecific challenges in addressing risks due to the way they may\\nemerge and materialise throughout the value chain.\\nAI Office\\xa0may invite GPAI model providers, relevant national\\ncompetent authorities to participate in drawing up the codes, while\\ncivil society, industry, academia, downstream providers and\\nindependent experts may support the process.\\nGovernance\\nHow will the AI Act be implemented?\\nThe\\xa0AI Office\\xa0will be established, sitting within the Commission, to\\nmonitor the effective implementation and compliance of GPAI\\nmodel providers.\\nDownstream providers can lodge a complaint regarding the\\nupstream providers infringement to the AI Office.\\nThe AI Office may conduct evaluations of the GPAI model to:\\nassess compliance where the information gathered under its powers\\nto request information is insufficient.'), Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Investigate systemic risks, particularly following a qualified report\\nfrom the scientific panel of independent experts.\\nTimelines\\nAfter entry into force, the AI Act will apply by the following\\ndeadlines:\\n6 months for prohibited AI systems.\\n12 months for GPAI.\\xa0\\n24 months for high risk AI systems under Annex III.\\xa0\\n36 months for high risk AI systems under Annex I.')]\n"
     ]
    }
   ],
   "source": [
    "print(text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75182f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for extracted images\n",
    "\n",
    "for doc in text_docs:\n",
    "    if 'images' in doc.metadata:\n",
    "        images = doc.metadata['images']\n",
    "        print(f\"Extracted {len(images)} images on this page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b865480e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='High-level summary of the AI Act\\n27 Feb, 2024\\nUpdated on 30 May in accordance with the Corrigendum version of\\nthe AI Act.\\nIn this article we provide you with a high-level summary of the AI\\nAct, selecting the parts which are most likely to be relevant to you\\nregardless of who you are. We provide links to the original\\ndocument where relevant so that you can always reference the Act\\ntext.\\nTo explore the full text of the AI Act yourself, use our\\xa0AI Act\\nExplorer. Alternatively, if you want to know which parts of the text\\nare most relevant to you, use our\\xa0Compliance Checker.\\nFour-point summary\\nThe AI Act classifies AI according to its risk:\\nUnacceptable risk is prohibited (e.g. social scoring systems and\\nmanipulative AI).\\nMost of the text addresses high-risk AI systems, which are\\nregulated.\\nA smaller section handles limited risk AI systems, subject to lighter\\ntransparency obligations: developers and deployers must ensure\\nthat end-users are aware that they are interacting with AI (chatbots'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='transparency obligations: developers and deployers must ensure\\nthat end-users are aware that they are interacting with AI (chatbots\\nand deepfakes).\\nMinimal risk is unregulated (including the majority of AI\\napplications currently available on the EU single market, such as AI\\nenabled video games and spam filters – at least in 2021; this is\\nchanging with generative AI).\\nThe majority of obligations fall on providers (developers) of high-\\nrisk AI systems.\\nThose that intend to place on the market or put into service high-'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='risk AI systems in the EU, regardless of whether they are based in\\nthe EU or a third country.\\nAnd also third country providers where the high risk AI system’s\\noutput is used in the EU.\\nUsers are natural or legal persons that deploy an AI system in a\\nprofessional capacity, not affected end-users.\\nUsers (deployers) of high-risk AI systems have some obligations,\\nthough less than providers (developers).\\nThis applies to users located in the EU, and third country users\\nwhere the AI system’s output is used in the EU.\\nGeneral purpose AI (GPAI):\\nAll GPAI model providers must provide technical documentation,\\ninstructions for use, comply with the Copyright Directive, and\\npublish a summary about the content used for training.\\nFree and open licence GPAI model providers only need to comply\\nwith copyright and publish the training data summary, unless they\\npresent a systemic risk.\\nAll providers of GPAI models that present a systemic risk – open or'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='with copyright and publish the training data summary, unless they\\npresent a systemic risk.\\nAll providers of GPAI models that present a systemic risk – open or\\nclosed – must also conduct model evaluations, adversarial testing,\\ntrack and report serious incidents and ensure cybersecurity\\nprotections.\\nProhibited AI systems (Chapter II,\\xa0Art. 5)\\nThe following types of AI system are ‘Prohibited’ according to the\\nAI Act.\\nAI systems:\\ndeploying\\xa0subliminal, manipulative, or deceptive techniques\\xa0to\\ndistort behaviour and impair informed decision-making, causing\\nsignificant harm.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'format': 'Office document', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='exploiting vulnerabilities\\xa0related to age, disability, or socio-\\neconomic circumstances to distort behaviour, causing significant\\nharm.\\nbiometric categorisation systems\\xa0inferring sensitive attributes (race,\\npolitical opinions, trade union membership, religious or\\nphilosophical beliefs, sex life, or sexual orientation), except\\nlabelling or filtering of lawfully acquired biometric datasets or\\nwhen law enforcement categorises biometric data.\\nsocial scoring, i.e., evaluating or classifying individuals or groups\\nbased on social behaviour or personal traits, causing detrimental or\\nunfavourable treatment of those people.\\nassessing the risk of an individual committing criminal\\noffenses\\xa0solely based on profiling or personality traits, except when\\nused to augment human assessments based on objective, verifiable\\nfacts directly linked to criminal activity.\\ncompiling facial recognition databases\\xa0by untargeted scraping of\\nfacial images from the internet or CCTV footage.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Splittings \n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "docs = text_splitter.split_documents(text_docs)\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e9c2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Embeddings and Vectorstore\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(docs, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b875cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-level summary of the AI Act\n",
      "27 Feb, 2024\n",
      "Updated on 30 May in accordance with the Corrigendum version of\n",
      "the AI Act.\n",
      "In this article we provide you with a high-level summary of the AI\n",
      "Act, selecting the parts which are most likely to be relevant to you\n",
      "regardless of who you are. We provide links to the original\n",
      "document where relevant so that you can always reference the Act\n",
      "text.\n",
      "To explore the full text of the AI Act yourself, use our AI Act\n",
      "Explorer. Alternatively, if you want to know which parts of the text\n",
      "are most relevant to you, use our Compliance Checker.\n",
      "Four-point summary\n",
      "The AI Act classifies AI according to its risk:\n",
      "Unacceptable risk is prohibited (e.g. social scoring systems and\n",
      "manipulative AI).\n",
      "Most of the text addresses high-risk AI systems, which are\n",
      "regulated.\n",
      "A smaller section handles limited risk AI systems, subject to lighter\n",
      "transparency obligations: developers and deployers must ensure\n",
      "that end-users are aware that they are interacting with AI (chatbots\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the four point summary?\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaecc902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000021DE6D97B50> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000021DE7B3B690> root_client=<openai.OpenAI object at 0x0000021DE6E0C890> root_async_client=<openai.AsyncOpenAI object at 0x0000021DE6A6E290> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b849ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            Answer the following question based only on the context. \n",
    "            Think step by step defore providing a detailed answer. \n",
    "            I will tip you $1000 if the used finds the answer helpful.\n",
    "            Context : {context}\n",
    "            Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49e58351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000021DE5757290>, search_kwargs={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96cf01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b1610d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "198d84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = retrieval_chain.invoke({\"input\":\"An attention function can be described as mapping a query\"})\n",
    "response = retrieval_chain.invoke({\"input\":\"What is the four point summary?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd03121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the four point summary?',\n",
       " 'context': [Document(metadata={'moddate': '', 'creationDate': '', 'subject': '', 'producer': '', 'creator': '', 'encryption': '', 'author': '', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'format': 'Office document', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'modDate': '', 'title': '', 'creationdate': '', 'trapped': '', 'keywords': '', 'page': 0, 'total_pages': 10}, page_content='High-level summary of the AI Act\\n27 Feb, 2024\\nUpdated on 30 May in accordance with the Corrigendum version of\\nthe AI Act.\\nIn this article we provide you with a high-level summary of the AI\\nAct, selecting the parts which are most likely to be relevant to you\\nregardless of who you are. We provide links to the original\\ndocument where relevant so that you can always reference the Act\\ntext.\\nTo explore the full text of the AI Act yourself, use our\\xa0AI Act\\nExplorer. Alternatively, if you want to know which parts of the text\\nare most relevant to you, use our\\xa0Compliance Checker.\\nFour-point summary\\nThe AI Act classifies AI according to its risk:\\nUnacceptable risk is prohibited (e.g. social scoring systems and\\nmanipulative AI).\\nMost of the text addresses high-risk AI systems, which are\\nregulated.\\nA smaller section handles limited risk AI systems, subject to lighter\\ntransparency obligations: developers and deployers must ensure\\nthat end-users are aware that they are interacting with AI (chatbots'),\n",
       "  Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx', 'creationdate': '', 'keywords': '', 'format': 'Office document', 'page': 7, 'creator': '', 'title': '', 'modDate': '', 'creationDate': '', 'moddate': '', 'subject': '', 'producer': '', 'trapped': '', 'total_pages': 10, 'encryption': '', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'author': ''}, page_content='GPAI models present systemic risks when the cumulative amount of\\ncompute used for its training is greater than 1025\\xa0floating point\\noperations (FLOPs). Providers must notify the Commission if their\\nmodel meets this criterion within 2 weeks. The provider may\\npresent arguments that, despite meeting the criteria, their model\\ndoes not present systemic risks. The Commission may decide on its\\nown, or via a qualified alert from the scientific panel of\\nindependent experts, that a model has high impact capabilities,\\nrendering it systemic.\\nIn addition to the four obligations above, providers of GPAI models\\nwith systemic risk must also:\\nPerform\\xa0model evaluations, including conducting and\\ndocumenting\\xa0adversarial testing\\xa0to identify and mitigate systemic\\nrisk.\\nAssess and mitigate possible systemic risks, including their sources.\\nTrack, document and report serious incidents\\xa0and possible\\ncorrective measures to the\\xa0AI Office\\xa0and relevant national\\ncompetent authorities without undue delay.'),\n",
       "  Document(metadata={'creator': '', 'title': '', 'subject': '', 'creationdate': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'total_pages': 10, 'page': 7, 'keywords': '', 'creationDate': '', 'encryption': '', 'author': '', 'modDate': '', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'moddate': '', 'trapped': '', 'producer': '', 'format': 'Office document'}, page_content='Draw up\\xa0technical documentation, including training and testing\\nprocess and evaluation results.\\nDraw up\\xa0information and documentation to supply to downstream\\nproviders\\xa0that intend to integrate the GPAI model into their own AI\\nsystem in order that the latter understands capabilities and\\nlimitations and is enabled to comply.\\nEstablish a policy to\\xa0respect the Copyright Directive.\\nPublish a\\xa0sufficiently detailed summary about the content used for\\ntraining\\xa0the GPAI model.\\nFree and open licence GPAI models\\xa0– whose parameters, including\\nweights, model architecture and model usage are publicly available,\\nallowing for access, usage, modification and distribution of the\\nmodel – only have to comply with the latter two obligations above,\\nunless the free and open licence GPAI model is systemic.\\nGPAI models present systemic risks when the cumulative amount of\\ncompute used for its training is greater than 1025\\xa0floating point\\noperations (FLOPs). Providers must notify the Commission if their'),\n",
       "  Document(metadata={'author': '', 'encryption': '', 'creator': '', 'modDate': '', 'format': 'Office document', 'creationDate': '', 'page': 1, 'title': '', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'keywords': '', 'total_pages': 10, 'source': '../Data/EU AI Act Doc (1) (3).docx', 'trapped': '', 'creationdate': '', 'subject': '', 'producer': '', 'moddate': ''}, page_content='with copyright and publish the training data summary, unless they\\npresent a systemic risk.\\nAll providers of GPAI models that present a systemic risk – open or\\nclosed – must also conduct model evaluations, adversarial testing,\\ntrack and report serious incidents and ensure cybersecurity\\nprotections.\\nProhibited AI systems (Chapter II,\\xa0Art. 5)\\nThe following types of AI system are ‘Prohibited’ according to the\\nAI Act.\\nAI systems:\\ndeploying\\xa0subliminal, manipulative, or deceptive techniques\\xa0to\\ndistort behaviour and impair informed decision-making, causing\\nsignificant harm.')],\n",
       " 'answer': 'To determine the four-point summary based on the provided context regarding the AI Act, we\\'ll break down the relevant information step by step:\\n\\n1. **Classification of AI Based on Risk:**\\n   - The AI Act classifies AI systems according to their risk level.\\n   - Systems deemed as \"unacceptable risk,\" such as those involving social scoring and manipulative AI, are prohibited.\\n\\n2. **Regulation of High-risk AI Systems:**\\n   - The majority of the AI Act\\'s text is dedicated to addressing high-risk AI systems.\\n   - These systems are subjected to strict regulations to ensure safety and compliance.\\n\\n3. **Handling of Limited Risk AI Systems:**\\n   - AI systems with a limited risk are subject to lighter transparency obligations.\\n   - Developers and deployers must ensure end-users are aware they are interacting with AI, for instance in the case of chatbots.\\n\\n4. **Systemic Risk Notification and Obligations for GPAI Models:**\\n   - GPAI models are considered to have systemic risks when their training exceeds 10^25 floating point operations (FLOPs).\\n   - Providers must notify the Commission if their model meets this threshold.\\n   - Providers of systemic risk GPAI models have further obligations, such as conducting model evaluations, adversarial testing, tracking incidents, and ensuring copyright compliance.\\n\\nThis summary captures the key elements of the AI Act based on the provided context.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "085288df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the four-point summary based on the provided context regarding the AI Act, we'll break down the relevant information step by step:\n",
      "\n",
      "1. **Classification of AI Based on Risk:**\n",
      "   - The AI Act classifies AI systems according to their risk level.\n",
      "   - Systems deemed as \"unacceptable risk,\" such as those involving social scoring and manipulative AI, are prohibited.\n",
      "\n",
      "2. **Regulation of High-risk AI Systems:**\n",
      "   - The majority of the AI Act's text is dedicated to addressing high-risk AI systems.\n",
      "   - These systems are subjected to strict regulations to ensure safety and compliance.\n",
      "\n",
      "3. **Handling of Limited Risk AI Systems:**\n",
      "   - AI systems with a limited risk are subject to lighter transparency obligations.\n",
      "   - Developers and deployers must ensure end-users are aware they are interacting with AI, for instance in the case of chatbots.\n",
      "\n",
      "4. **Systemic Risk Notification and Obligations for GPAI Models:**\n",
      "   - GPAI models are considered to have systemic risks when their training exceeds 10^25 floating point operations (FLOPs).\n",
      "   - Providers must notify the Commission if their model meets this threshold.\n",
      "   - Providers of systemic risk GPAI models have further obligations, such as conducting model evaluations, adversarial testing, tracking incidents, and ensuring copyright compliance.\n",
      "\n",
      "This summary captures the key elements of the AI Act based on the provided context.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24889fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previously completed human assessment without proper human\n",
      "review; or\n",
      "performs a preparatory task to an assessment relevant for the\n",
      "purpose of the use cases listed in Annex III.\n",
      "AI systems are always considered high-risk if it profiles individuals,\n",
      "i.e. automated processing of personal data to assess various aspects\n",
      "of a person’s life, such as work performance, economic situation,\n",
      "health, preferences, interests, reliability, behaviour, location or\n",
      "movement.\n",
      "Providers whose AI system falls under the use cases in Annex III but\n",
      "believes it is not high-risk must document such an assessment\n",
      "before placing it on the market or putting it into service.\n",
      "Requirements for providers of high-risk AI systems (Art. 8–17)\n",
      "High risk AI providers must:\n",
      "Establish a risk management system throughout the high risk AI\n",
      "system’s lifecycle;\n",
      "Conduct data governance, ensuring that training, validation and\n",
      "testing datasets are relevant, sufficiently representative and, to the\n"
     ]
    }
   ],
   "source": [
    "# query = \"What is multi head attention?\"\n",
    "\n",
    "query = \"Requirements for providers of high-risk AI systems\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "671f5268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../Data/EU AI Act Doc (1) (3).docx', 'producer': '', 'author': '', 'page': 3, 'creator': '', 'modDate': '', 'format': 'Office document', 'creationdate': '', 'moddate': '', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'keywords': '', 'total_pages': 10, 'subject': '', 'trapped': '', 'encryption': '', 'creationDate': '', 'title': ''}, page_content='greater political influence than judicial authorities (Hacker, 2024).\\nHigh risk AI systems (Chapter III)\\nSome AI systems are considered ‘High risk’ under the AI Act.\\nProviders of those systems will be subject to additional\\nrequirements.\\nClassification rules for high-risk AI systems (Art. 6)\\nHigh risk AI systems are those:\\nused as a safety component or a product covered by EU laws\\nin\\xa0Annex I\\xa0AND\\xa0required to undergo a third-party conformity\\nassessment under those\\xa0Annex I\\xa0laws;\\xa0OR\\nthose under\\xa0Annex III\\xa0use cases (below), except if:\\nthe AI system performs a narrow procedural task;\\nimproves the result of a previously completed human activity;\\ndetects decision-making patterns or deviations from prior decision-\\nmaking patterns and is not meant to replace or influence the'),\n",
       " Document(metadata={'subject': '', 'creator': '', 'title': '', 'moddate': '', 'keywords': '', 'author': '', 'creationdate': '', 'producer': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'format': 'Office document', 'total_pages': 10, 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'trapped': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'page': 4}, page_content='previously completed human assessment without proper human\\nreview; or\\nperforms a preparatory task to an assessment relevant for the\\npurpose of the use cases listed in Annex III.\\nAI systems are always considered high-risk if it profiles individuals,\\ni.e. automated processing of personal data to assess various aspects\\nof a person’s life, such as work performance, economic situation,\\nhealth, preferences, interests, reliability, behaviour, location or\\nmovement.\\nProviders whose AI system falls under the use cases in\\xa0Annex III\\xa0but\\nbelieves it is\\xa0not\\xa0high-risk must document such an assessment\\nbefore placing it on the market or putting it into service.\\nRequirements for providers of high-risk AI systems (Art.\\xa08–17)\\nHigh risk AI providers must:\\nEstablish a\\xa0risk management system\\xa0throughout the high risk AI\\nsystem’s lifecycle;\\nConduct\\xa0data governance, ensuring that training, validation and\\ntesting datasets are relevant, sufficiently representative and, to the'),\n",
       " Document(metadata={'modDate': '', 'creationDate': '', 'producer': '', 'subject': '', 'author': '', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'creationdate': '', 'total_pages': 10, 'page': 1, 'title': '', 'encryption': '', 'format': 'Office document', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'creator': '', 'moddate': '', 'trapped': '', 'keywords': ''}, page_content='risk AI systems in the EU, regardless of whether they are based in\\nthe EU or a third country.\\nAnd also third country providers where the high risk AI system’s\\noutput is used in the EU.\\nUsers are natural or legal persons that deploy an AI system in a\\nprofessional capacity, not affected end-users.\\nUsers (deployers) of high-risk AI systems have some obligations,\\nthough less than providers (developers).\\nThis applies to users located in the EU, and third country users\\nwhere the AI system’s output is used in the EU.\\nGeneral purpose AI (GPAI):\\nAll GPAI model providers must provide technical documentation,\\ninstructions for use, comply with the Copyright Directive, and\\npublish a summary about the content used for training.\\nFree and open licence GPAI model providers only need to comply\\nwith copyright and publish the training data summary, unless they\\npresent a systemic risk.\\nAll providers of GPAI models that present a systemic risk – open or'),\n",
       " Document(metadata={'author': '', 'moddate': '', 'producer': '', 'creator': '', 'title': '', 'format': 'Office document', 'keywords': '', 'modDate': '', 'creationdate': '', 'file_path': '../Data/EU AI Act Doc (1) (3).docx', 'encryption': '', 'source': '../Data/EU AI Act Doc (1) (3).docx', 'trapped': '', 'total_pages': 10, 'subject': '', 'page': 1, 'creationDate': ''}, page_content='with copyright and publish the training data summary, unless they\\npresent a systemic risk.\\nAll providers of GPAI models that present a systemic risk – open or\\nclosed – must also conduct model evaluations, adversarial testing,\\ntrack and report serious incidents and ensure cybersecurity\\nprotections.\\nProhibited AI systems (Chapter II,\\xa0Art. 5)\\nThe following types of AI system are ‘Prohibited’ according to the\\nAI Act.\\nAI systems:\\ndeploying\\xa0subliminal, manipulative, or deceptive techniques\\xa0to\\ndistort behaviour and impair informed decision-making, causing\\nsignificant harm.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retireved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c155243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceadar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
